"""
Centralized scoring logic for the ICP project.

This module contains all the functions and configurations required to calculate
the Ideal Customer Profile (ICP) scores. It defines the default weights,
the data-driven scoring logic for each component (vertical, size, adoption, relationship),
the final score normalization process, and the grading system.
"""

import pandas as pd
import numpy as np
import json
import os
from scipy.stats import norm

# --- Constants and Configurations ---
LICENSE_COL = "Total Software License Revenue"

# Default weights for ICP scoring.
# These are used as a fallback if the `optimized_weights.json` file is not found.
DEFAULT_WEIGHTS = {
    "vertical": 0.3,
    "size": 0.3,
    "adoption": 0.2,
    "relationship": 0.2,
}

# Defines the target distribution for the final A-F grades.
# For example, the top 10% of customers should receive an 'A' grade.
TARGET_GRADE_DISTRIBUTION = {
    'A': 0.10,  # Top 10%
    'B': 0.20,  # Next 20%
    'C': 0.40,  # Middle 40%
    'D': 0.20,  # Next 20%
    'F': 0.10   # Bottom 10%
}
# Cumulative distribution for easier grade assignment based on percentile ranks.
TARGET_CUMULATIVE_DISTRIBUTION = np.cumsum([
    TARGET_GRADE_DISTRIBUTION['F'],
    TARGET_GRADE_DISTRIBUTION['D'],
    TARGET_GRADE_DISTRIBUTION['C'],
    TARGET_GRADE_DISTRIBUTION['B'],
    TARGET_GRADE_DISTRIBUTION['A']
])

# Data-driven vertical weights based on the historical revenue performance of each industry.
# Higher values indicate better historical performance.
PERFORMANCE_VERTICAL_WEIGHTS = {
    "aerospace & defense": 1.0,
    "automotive & transportation": 1.0,
    "consumer goods": 1.0,
    "high tech": 1.0,
    "medical devices & life sciences": 1.0,
    "engineering services": 0.8,
    "heavy equip & ind. components": 0.8,
    "industrial machinery": 0.8,
    "mold, tool & die": 0.8,
    "other": 0.8,
    "building & construction": 0.6,
    "chemicals & related products": 0.6,
    "dental": 0.6,
    "manufactured products": 0.6,
    "services": 0.6,
    "education & research": 0.4,
    "electromagnetic": 0.4,
    "energy": 0.4,
    "packaging": 0.4,
    "plant & process": 0.4,
    "shipbuilding": 0.4,
}

def load_dynamic_industry_weights():
    """
    Load industry weights from the JSON file generated by industry_scoring.py.
    Falls back to static weights if file doesn't exist.
    
    Returns:
        dict: Industry name -> score mapping
    """
    industry_weights_file = "industry_weights.json"
    
    if os.path.exists(industry_weights_file):
        try:
            with open(industry_weights_file, 'r') as f:
                data = json.load(f)
            weights = data.get('weights', PERFORMANCE_VERTICAL_WEIGHTS)
            print(f"[INFO] Loaded {len(weights)} dynamic industry weights")
            return weights
        except Exception as e:
            print(f"[WARN] Error loading dynamic industry weights: {e}")
            print("[WARN] Falling back to static weights")
            return PERFORMANCE_VERTICAL_WEIGHTS
    else:
        print("[INFO] No dynamic industry weights found, using static weights")
        return PERFORMANCE_VERTICAL_WEIGHTS

def calculate_grades(scores):
    """
    Assigns A-F grades based on the percentile rank of the final scores.

    Args:
        scores (pd.Series): A series of final, normalized ICP scores.

    Returns:
        np.ndarray: An array of corresponding letter grades ('A' through 'F').
    """
    ranks = scores.rank(pct=True)
    grades = np.select(
        [
            ranks <= TARGET_CUMULATIVE_DISTRIBUTION[0], # F
            ranks <= TARGET_CUMULATIVE_DISTRIBUTION[1], # D
            ranks <= TARGET_CUMULATIVE_DISTRIBUTION[2], # C
            ranks <= TARGET_CUMULATIVE_DISTRIBUTION[3], # B
            ranks > TARGET_CUMULATIVE_DISTRIBUTION[3]  # A
        ],
        ['F', 'D', 'C', 'B', 'A'],
        default='C'
    )
    return grades

def calculate_scores(df, weights, size_config=None):
    """
    Calculates all component scores, the raw ICP score, the final normalized
    ICP score, and the grade for each customer in the DataFrame.

    This is the core function of the scoring engine. It applies data-driven logic
    for each of the four main scoring criteria.

    Args:
        df (pd.DataFrame): The input DataFrame containing customer data.
        weights (dict): A dictionary of weights for each of the four components.
        size_config (dict, optional): Kept for legacy compatibility but is no longer used
                                      as the size scoring is now hardcoded.

    Returns:
        pd.DataFrame: The DataFrame with added columns for each score component,
                      the raw score, the final score, and the grade.
    """
    df_clean = df.copy()

    # --- Pre-emptive Cleanup ---
    # Drop any pre-existing score columns to prevent errors on re-calculation,
    # which is important for the interactive Streamlit dashboard.
    score_cols_to_drop = [
        'vertical_score', 'size_score', 'adoption_score', 'relationship_score', 
        'relationship_feature', 'ICP_score_raw', 'ICP_score'
    ]
    for col in score_cols_to_drop:
        if col in df_clean.columns:
            df_clean = df_clean.drop(columns=col)
    
    # 1. Vertical Score: Based on a direct mapping from the industry name to its
    #    data-driven performance weight (loaded dynamically from industry_weights.json).
    industry_weights = load_dynamic_industry_weights()
    v_lower = df_clean["Industry"].astype(str).str.lower().str.strip()
    df_clean["vertical_score"] = v_lower.map(industry_weights).fillna(0.3)

    # 2. Size Score: Based on tiers of enriched annual revenue data.
    #    A neutral default score is assigned, then updated based on revenue brackets.
    revenue_values = df_clean['revenue_estimate'].fillna(0)
    has_reliable_revenue = revenue_values > 0
    
    df_clean["size_score"] = 0.5  # Neutral default score
    
    conditions = [
        (revenue_values >= 250_000_000) & (revenue_values < 1_000_000_000), # $250M - $1B
        (revenue_values >= 1_000_000_000),                                # > $1B
        (revenue_values >= 50_000_000),                                 # > $50M
        (revenue_values >= 10_000_000),                                 # > $10M
        (revenue_values > 0)
    ]
    scores = [1.0, 0.9, 0.6, 0.4, 0.4] # Corresponding scores for the conditions
    
    for condition, score in zip(conditions, scores):
        mask = has_reliable_revenue & condition
        df_clean.loc[mask, "size_score"] = score

    # 3. Adoption Score: A composite score of printer count and consumable revenue.
    #    - Both features are log-transformed to handle skewed distributions.
    #    - They are then scaled to a 0-1 range using min-max scaling.
    #    - Finally, they are combined with a 50/50 weighting.
    def min_max_scale(series):
        min_val, max_val = series.min(), series.max()
        if max_val - min_val == 0:
            return pd.Series(0.0, index=series.index)
        return (series - min_val) / (max_val - min_val)

    if 'Total Consumable Revenue' not in df_clean.columns:
        df_clean['Total Consumable Revenue'] = 0

    printer_count_safe = np.maximum(df_clean['printer_count'].fillna(0), 0)
    consumable_revenue_safe = np.maximum(df_clean['Total Consumable Revenue'].fillna(0), 0)
    
    printer_score = min_max_scale(np.log1p(printer_count_safe))
    consumable_score = min_max_scale(np.log1p(consumable_revenue_safe))
    df_clean['adoption_score'] = 0.5 * printer_score + 0.5 * consumable_score

    # 4. Relationship Score: Based on the sum of all software-related revenue.
    #    - The total software revenue is log-transformed and then min-max scaled.
    relationship_cols = ['Total Software License Revenue', 'Total SaaS Revenue', 'Total Maintenance Revenue']
    for col in relationship_cols:
        if col not in df_clean.columns:
            df_clean[col] = 0
    df_clean['relationship_feature'] = df_clean[relationship_cols].fillna(0).sum(axis=1)
    relationship_feature_safe = np.maximum(df_clean['relationship_feature'], 0)
    df_clean['relationship_score'] = min_max_scale(np.log1p(relationship_feature_safe))
    
    # This creates the 'cad_tier' for display and filtering purposes (legacy feature).
    if LICENSE_COL in df_clean.columns:
        license_revenue = pd.to_numeric(df_clean[LICENSE_COL], errors='coerce').fillna(0)
        bins = [-1, 5000, 25000, 100000, np.inf]
        labels = ["Bronze", "Silver", "Gold", "Platinum"]
        df_clean['cad_tier'] = pd.cut(license_revenue, bins=bins, labels=labels)
    else:
        df_clean['cad_tier'] = 'Bronze'
        df_clean['cad_tier'] = pd.Categorical(df_clean['cad_tier'], categories=["Bronze", "Silver", "Gold", "Platinum"])

    # --- Final Score Calculation ---

    # Calculate the raw, weighted ICP score.
    df_clean['ICP_score_raw'] = (
        weights["vertical"] * df_clean['vertical_score'] +
        weights["size"] * df_clean['size_score'] +
        weights["adoption"] * df_clean['adoption_score'] +
        weights["relationship"] * df_clean['relationship_score']
    ) * 100

    # Monotonic Normalization: Transform the raw scores to a bell-curve distribution.
    # This makes the final scores more intuitive and comparable.
    # 1. Rank the scores.
    # 2. Convert ranks to percentiles.
    # 3. Use the inverse of the normal CDF (ppf) to map percentiles to a normal distribution.
    ranks = df_clean['ICP_score_raw'].rank(method='first')
    n = len(ranks)
    p = (ranks - 0.5) / n
    z = norm.ppf(p)

    # Scale the Z-scores to a 0-100 range with a mean of 50 and std dev of 15.
    df_clean['ICP_score'] = (50 + 15 * z).clip(0, 100)
    
    # Assign letter grades based on the final normalized ICP score percentiles.
    df_clean['ICP_grade'] = calculate_grades(df_clean['ICP_score'])
    
    return df_clean
