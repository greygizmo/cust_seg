"""
Centralized scoring logic for the ICP project.

This module contains all the functions and configurations required to calculate
the Ideal Customer Profile (ICP) scores. It defines the default weights,
the data-driven scoring logic for each component (vertical, size, adoption, relationship),
the final score normalization process, and the grading system.
"""

import pandas as pd
import numpy as np
import json
import os
from pathlib import Path
from scipy.stats import norm

# --- Constants and Configurations ---
LICENSE_COL = "Total Software License Revenue"

# Default weights for ICP scoring.
# These are used as a fallback if the `optimized_weights.json` file is not found.
DEFAULT_WEIGHTS = {
    "vertical": 0.3,
    "size": 0.0,
    "adoption": 0.50,
    "relationship": 0.2,
}

# Defines the target distribution for the final A-F grades.
# For example, the top 10% of customers should receive an 'A' grade.
TARGET_GRADE_DISTRIBUTION = {
    'A': 0.10,  # Top 10%
    'B': 0.20,  # Next 20%
    'C': 0.40,  # Middle 40%
    'D': 0.20,  # Next 20%
    'F': 0.10   # Bottom 10%
}
# Cumulative distribution for easier grade assignment based on percentile ranks.
TARGET_CUMULATIVE_DISTRIBUTION = np.cumsum([
    TARGET_GRADE_DISTRIBUTION['F'],
    TARGET_GRADE_DISTRIBUTION['D'],
    TARGET_GRADE_DISTRIBUTION['C'],
    TARGET_GRADE_DISTRIBUTION['B'],
    TARGET_GRADE_DISTRIBUTION['A']
])

# Data-driven vertical weights based on the historical revenue performance of each industry.
# Higher values indicate better historical performance.
PERFORMANCE_VERTICAL_WEIGHTS = {
    "aerospace & defense": 1.0,
    "automotive & transportation": 1.0,
    "consumer goods": 1.0,
    "high tech": 1.0,
    "medical devices & life sciences": 1.0,
    "engineering services": 0.8,
    "heavy equip & ind. components": 0.8,
    "industrial machinery": 0.8,
    "mold, tool & die": 0.8,
    "other": 0.8,
    "building & construction": 0.6,
    "chemicals & related products": 0.6,
    "dental": 0.6,
    "manufactured products": 0.6,
    "services": 0.6,
    "education & research": 0.4,
    "electromagnetic": 0.4,
    "energy": 0.4,
    "packaging": 0.4,
    "plant & process": 0.4,
    "shipbuilding": 0.4,
}

def load_dynamic_industry_weights():
    """
    Load industry weights from the JSON file generated by industry_scoring.py.
    Falls back to static weights if file doesn't exist.
    
    Returns:
        dict: Industry name -> score mapping
    """
    # Resolve repository root and candidate paths
    ROOT = Path(__file__).resolve().parents[2]
    candidates = [
        ROOT / 'artifacts' / 'weights' / 'industry_weights.json',
        Path.cwd() / 'industry_weights.json'
    ]
    for industry_weights_file in candidates:
        if industry_weights_file.exists():
            try:
                with open(industry_weights_file, 'r') as f:
                    data = json.load(f)
                weights = data.get('weights', PERFORMANCE_VERTICAL_WEIGHTS)
                print(f"[INFO] Loaded {len(weights)} dynamic industry weights from {industry_weights_file}")
                return weights
            except Exception as e:
                print(f"[WARN] Error loading dynamic industry weights: {e}")
                print("[WARN] Falling back to static weights")
                return PERFORMANCE_VERTICAL_WEIGHTS
    print("[INFO] No dynamic industry weights found, using static weights")
    return PERFORMANCE_VERTICAL_WEIGHTS

def calculate_grades(scores):
    """
    Assigns A-F grades based on the percentile rank of the final scores.

    Args:
        scores (pd.Series): A series of final, normalized ICP scores.

    Returns:
        np.ndarray: An array of corresponding letter grades ('A' through 'F').
    """
    ranks = scores.rank(pct=True)
    grades = np.select(
        [
            ranks <= TARGET_CUMULATIVE_DISTRIBUTION[0], # F
            ranks <= TARGET_CUMULATIVE_DISTRIBUTION[1], # D
            ranks <= TARGET_CUMULATIVE_DISTRIBUTION[2], # C
            ranks <= TARGET_CUMULATIVE_DISTRIBUTION[3], # B
            ranks > TARGET_CUMULATIVE_DISTRIBUTION[3]  # A
        ],
        ['F', 'D', 'C', 'B', 'A'],
        default='C'
    )
    return grades

def calculate_scores(df, weights, size_config=None):
    """
    Calculates all component scores, the raw ICP score, the final normalized
    ICP score, and the grade for each customer in the DataFrame.

    This is the core function of the scoring engine. It applies data-driven logic
    for each of the four main scoring criteria.

    Args:
        df (pd.DataFrame): The input DataFrame containing customer data.
        weights (dict): A dictionary of weights for each of the four components.
        size_config (dict, optional): Kept for legacy compatibility but is no longer used
                                      as the size scoring is now hardcoded.

    Returns:
        pd.DataFrame: The DataFrame with added columns for each score component,
                      the raw score, the final score, and the grade.
    """
    df_clean = df.copy()

    # --- Pre-emptive Cleanup ---
    # Drop any pre-existing score columns to prevent errors on re-calculation,
    # which is important for the interactive Streamlit dashboard.
    score_cols_to_drop = [
        'vertical_score', 'size_score', 'adoption_score', 'relationship_score', 
        'relationship_feature', 'ICP_score_raw', 'ICP_score'
    ]
    for col in score_cols_to_drop:
        if col in df_clean.columns:
            df_clean = df_clean.drop(columns=col)
    
    # 1. Vertical Score: Based on a direct mapping from the industry name to its
    #    data-driven performance weight (loaded dynamically from industry_weights.json).
    industry_weights = load_dynamic_industry_weights()
    v_lower = df_clean["Industry"].astype(str).str.lower().str.strip()
    df_clean["vertical_score"] = v_lower.map(industry_weights).fillna(0.3)

    # 2. Size Score: Based on tiers of enriched annual revenue data.
    #    A neutral default score is assigned, then updated based on revenue brackets.
    # Use the correct revenue column name from the dataset
    revenue_col = 'Total Hardware + Consumable Revenue'
    if revenue_col in df_clean.columns:
        revenue_values = df_clean[revenue_col].fillna(0)
    else:
        # Fallback if column doesn't exist
        revenue_values = df_clean.get('revenue_estimate', pd.Series([0] * len(df_clean))).fillna(0)
    has_reliable_revenue = revenue_values > 0
    
    df_clean["size_score"] = 0.5  # Neutral default score
    
    conditions = [
        (revenue_values >= 250_000_000) & (revenue_values < 1_000_000_000), # $250M - $1B
        (revenue_values >= 1_000_000_000),                                # > $1B
        (revenue_values >= 50_000_000),                                 # > $50M
        (revenue_values >= 10_000_000),                                 # > $10M
        (revenue_values > 0)
    ]
    scores = [1.0, 0.9, 0.6, 0.4, 0.4] # Corresponding scores for the conditions
    
    for condition, score in zip(conditions, scores):
        mask = has_reliable_revenue & condition
        df_clean.loc[mask, "size_score"] = score

    # 3. Adoption Score: Composite of asset/seat investment and hardware/training profit.
    #    New (DB) mode prefers columns:
    #      - adoption_assets (weighted seats/assets for focus divisions)
    #      - adoption_profit (Profit since 2023 for focus divisions)
    #    Falls back to legacy printer + revenue logic if not present.
    def percentile_scale(series):
        """
        Convert values to percentile ranks (0-1 range).
        
        CRITICAL FIX: When many values are zero (like 86% of revenue data),
        standard percentile ranking compresses non-zero values into a tiny range.
        This function excludes zeros from percentile calculation to provide
        better distribution among actual non-zero values.
        """
        if len(series.unique()) == 1:
            return pd.Series(0.5, index=series.index)  # All same value = 50th percentile
        
        # Create result series initialized to 0.0
        result = pd.Series(0.0, index=series.index)
        
        # Identify non-zero values
        non_zero_mask = series > 0
        non_zero_values = series[non_zero_mask]
        
        if len(non_zero_values) > 0:
            # Calculate percentiles only among non-zero values
            non_zero_percentiles = non_zero_values.rank(method='average', pct=True)
            result[non_zero_mask] = non_zero_percentiles
        
        # Zero values remain 0.0
        return result
    
    def min_max_scale(series):
        """Scale values to 0-1 range using min-max normalization"""
        min_val, max_val = series.min(), series.max()
        if max_val - min_val == 0:
            return pd.Series(0.0, index=series.index)
        return (series - min_val) / (max_val - min_val)

    if 'adoption_assets' in df_clean.columns and 'adoption_profit' in df_clean.columns:
        assets_series = pd.to_numeric(df_clean['adoption_assets'], errors='coerce').fillna(0)
        profit_series = pd.to_numeric(df_clean['adoption_profit'], errors='coerce').fillna(0)

        P = percentile_scale(assets_series)
        R = percentile_scale(profit_series)

        zero_assets = assets_series == 0
        zero_profit = profit_series == 0

        adoption_scores = np.zeros(len(df_clean))
        # Profit-only customers: 0.0-0.5 via sqrt
        profit_only_mask = zero_assets & ~zero_profit
        adoption_scores[profit_only_mask] = 0.5 * np.sqrt(R[profit_only_mask])
        # With assets: blend 60/40
        with_assets_mask = ~zero_assets
        adoption_scores[with_assets_mask] = (0.6 * P + 0.4 * R)[with_assets_mask]

        df_clean['adoption_score'] = adoption_scores
    else:
        # Legacy fallback using printer counts and hardware+consumable revenue
        if 'Total Consumable Revenue' not in df_clean.columns:
            df_clean['Total Consumable Revenue'] = 0
        if 'Total Hardware Revenue' not in df_clean.columns:
            df_clean['Total Hardware Revenue'] = 0

        big_box_safe = np.maximum(df_clean.get('Big Box Count', 0), 0)
        small_box_safe = np.maximum(df_clean.get('Small Box Count', 0), 0)
        weighted_printer_score = (2.0 * pd.to_numeric(big_box_safe, errors='coerce').fillna(0)) + (1.0 * pd.to_numeric(small_box_safe, errors='coerce').fillna(0))

        hardware_revenue_safe = np.maximum(pd.to_numeric(df_clean['Total Hardware Revenue'], errors='coerce').fillna(0), 0)
        consumable_revenue_safe = np.maximum(pd.to_numeric(df_clean['Total Consumable Revenue'], errors='coerce').fillna(0), 0)
        total_hardware_adoption_revenue = hardware_revenue_safe + consumable_revenue_safe

        P = percentile_scale(weighted_printer_score)
        R = percentile_scale(total_hardware_adoption_revenue)

        zero_printer_mask = weighted_printer_score == 0
        zero_revenue_mask = total_hardware_adoption_revenue == 0

        adoption_scores = np.zeros(len(df_clean))
        revenue_only_mask = zero_printer_mask & ~zero_revenue_mask
        adoption_scores[revenue_only_mask] = 0.5 * np.sqrt(R[revenue_only_mask])
        with_printers_mask = ~zero_printer_mask
        base_score = 0.6 * P + 0.4 * R
        adoption_scores[with_printers_mask] = base_score[with_printers_mask]
        heavy_fleet_mask = weighted_printer_score >= 10
        adoption_scores[heavy_fleet_mask] = np.clip(adoption_scores[heavy_fleet_mask] + 0.05, 0, 1)
        df_clean['adoption_score'] = adoption_scores

    # 4. Relationship Score: prefer profit from software goals if present.
    if 'relationship_profit' in df_clean.columns:
        rel_safe = np.maximum(pd.to_numeric(df_clean['relationship_profit'], errors='coerce').fillna(0), 0)
        df_clean['relationship_score'] = min_max_scale(np.log1p(rel_safe))
    else:
        relationship_cols = ['Total Software License Revenue', 'Total SaaS Revenue', 'Total Maintenance Revenue']
        for col in relationship_cols:
            if col not in df_clean.columns:
                df_clean[col] = 0
        df_clean['relationship_feature'] = df_clean[relationship_cols].fillna(0).sum(axis=1)
        relationship_feature_safe = np.maximum(df_clean['relationship_feature'], 0)
        df_clean['relationship_score'] = min_max_scale(np.log1p(relationship_feature_safe))
    
    # This creates the 'cad_tier' for display and filtering purposes (legacy feature).
    if LICENSE_COL in df_clean.columns:
        license_revenue = pd.to_numeric(df_clean[LICENSE_COL], errors='coerce').fillna(0)
        bins = [-1, 5000, 25000, 100000, np.inf]
        labels = ["Bronze", "Silver", "Gold", "Platinum"]
        df_clean['cad_tier'] = pd.cut(license_revenue, bins=bins, labels=labels)
    else:
        df_clean['cad_tier'] = 'Bronze'
        df_clean['cad_tier'] = pd.Categorical(df_clean['cad_tier'], categories=["Bronze", "Silver", "Gold", "Platinum"])

    # --- Final Score Calculation ---

    # Calculate the raw, weighted ICP score.
    df_clean['ICP_score_raw'] = (
        weights["vertical"] * df_clean['vertical_score'] +
        weights["size"] * df_clean['size_score'] +
        weights["adoption"] * df_clean['adoption_score'] +
        weights["relationship"] * df_clean['relationship_score']
    ) * 100

    # Monotonic Normalization: Transform the raw scores to a bell-curve distribution.
    # This makes the final scores more intuitive and comparable.
    # 1. Rank the scores.
    # 2. Convert ranks to percentiles.
    # 3. Use the inverse of the normal CDF (ppf) to map percentiles to a normal distribution.
    ranks = df_clean['ICP_score_raw'].rank(method='first')
    n = len(ranks)
    p = (ranks - 0.5) / n
    z = norm.ppf(p)

    # Scale the Z-scores to a 0-100 range with a mean of 50 and std dev of 15.
    df_clean['ICP_score'] = (50 + 15 * z).clip(0, 100)
    
    # Assign letter grades based on the final normalized ICP score percentiles.
    df_clean['ICP_grade'] = calculate_grades(df_clean['ICP_score'])
    
    return df_clean
